{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f3148a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The plan is to:\n",
    "\n",
    "* prepare the data for modeling\n",
    "  * encode intent labels\n",
    "  * represent text as vectors\n",
    "\n",
    "We need to represent the non-numerical text data as numbers since models don't operate on\n",
    "text directly. For the intent categories, we can simply encode the intent strings as integers,\n",
    "and decode the model prediction back to an intent string. For the utterances, we have many options:\n",
    "\n",
    "* simple frequency-based representations - bag of words, TF-IDF\n",
    "* early embedding models based on word/subword co-occurrence - word2vec (Google), GloVe (Stanford), fastText (Facebook)\n",
    "* newer deep embeddings models, more context-sensitive and based on weights of neural networks - ELMo, BERT, GPT\n",
    "* latest SOTA transformers models (see HuggingFace embeddings benchmarks leaderboard - https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "For this project we will keep things simple and see what results we can achieve using a simple\n",
    "frequency-based representation, TF-IDF. Term frequency captures how often a word\n",
    "appears in an utterance. More frequent words in a document tend to be more important.\n",
    "Inverse Document Frequency considers how much information the word provides. Words\n",
    "appearing in many utterances carry less weight (an inverse relationship).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import (\n",
    "    init_nb, PROCESSED_DATA_PATH, INTENT_LABELS_PATH, TFIDF_DATA_PATH,\n",
    "    TFIDF_VECTORIZER_PATH, null_preprocessor, spacy_tokenizer\n",
    ")\n",
    "\n",
    "init_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical intent label encoding\n",
    "\n",
    "df = pd.read_json(PROCESSED_DATA_PATH, orient='records', lines=True)\n",
    "\n",
    "# encode intent labels as numbers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['intent_str'])\n",
    "df.loc[:, 'intent'] = encoder.transform(df['intent_str'])\n",
    "# or fit/transform together:\n",
    "# df.loc[:, 'intent'] = LabelEncoder().fit_transform(df['intent_str'])\n",
    "\n",
    "# int labels\n",
    "int_labels = encoder.transform(encoder.classes_).tolist()\n",
    "\n",
    "# str labels\n",
    "str_labels = encoder.classes_.tolist()\n",
    "\n",
    "# save mapping\n",
    "intent_labels: DataFrame = pd.DataFrame(\n",
    "    {'label_int': int_labels,\n",
    "     'label_str': str_labels},\n",
    ")\n",
    "intent_labels.to_json(INTENT_LABELS_PATH)\n",
    "\n",
    "# verify number of utterances per intent\n",
    "print(df['intent'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0c582",
   "metadata": {},
   "source": [
    "```text\n",
    "intent\n",
    "28    159\n",
    "6     153\n",
    "1     150\n",
    "23    150\n",
    "17    150\n",
    "0     150\n",
    "18    150\n",
    "14    150\n",
    "26    150\n",
    "16    150\n",
    "25    150\n",
    "7     150\n",
    "13    150\n",
    "20    150\n",
    "24    150\n",
    "11    150\n",
    "22    150\n",
    "29    150\n",
    "27    150\n",
    "9     150\n",
    "15    150\n",
    "5     150\n",
    "19    150\n",
    "4     150\n",
    "10    150\n",
    "3     150\n",
    "2     150\n",
    "12    149\n",
    "21    149\n",
    "8     148\n",
    "Name: count, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504a78c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "vectorizer_kwargs = {\n",
    "    # if float, these parameters represent a proportion of documents,\n",
    "    # if integer they are absolute counts\n",
    "    'min_df': 1,  # ignore terms that have a document frequency strictly lower than the given threshold (default 1)\n",
    "    'max_df': 0.95, # ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words, default 1.0)\n",
    "    'analyzer': 'word', # 'char', char_wb', 'word' (default)\n",
    "    'ngram_range': (1, 2), # count unigrams and bigrams\n",
    "    'preprocessor': null_preprocessor, # text is already preprocessed\n",
    "    'tokenizer': spacy_tokenizer, # we already tokenized the text, but it's easiest to redo it here using a function\n",
    "    'token_pattern': None # since we are using spacy's tokenizer\n",
    "}\n",
    "\n",
    "# note: TfidfVectorizer combines CountVectorizer and TfidfTransformer\n",
    "tfidf = TfidfVectorizer(use_idf=True, smooth_idf=True, **vectorizer_kwargs)\n",
    "print('fitting tfidf vectorizer...')\n",
    "tfidf_v = tfidf.fit_transform(df.utterance)\n",
    "\n",
    "# save vectorizer for inference in the next notebook\n",
    "timestamp = int(time.time())\n",
    "tfidf_path = TFIDF_VECTORIZER_PATH.parent / f'{TFIDF_VECTORIZER_PATH.stem}_{timestamp}{TFIDF_VECTORIZER_PATH.suffix}'\n",
    "print(f'saving tfidf vectorizer to {tfidf_path}')\n",
    "joblib.dump(tfidf, tfidf_path)\n",
    "\n",
    "# We could've built this vectorizer into a sklearn Pipeline with the classifier.\n",
    "# That would've allowed us to vary the vectorizer parameters in the grid search.\n",
    "# I checked the ngram_range manually and found using unigrams and bigrams performed best.\n",
    "\n",
    "tfidf_data_path = TFIDF_DATA_PATH.parent / f'{TFIDF_DATA_PATH.stem}_{timestamp}{TFIDF_DATA_PATH.suffix}'\n",
    "df.loc[:, 'tfidf_vector'] = DataFrame(tfidf_v.toarray()).apply(lambda row: list(row.values), axis=1)\n",
    "df.to_json(tfidf_data_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f93c2",
   "metadata": {},
   "source": [
    "```text\n",
    "fitting tfidf vectorizer...\n",
    "saving tfidf vectorizer...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
