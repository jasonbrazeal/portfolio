{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649e7867",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The plans is to run the evaluations using our newly created dataset while varying\n",
    "several factors:\n",
    "  * basic (vector) vs. advanced (vector + bm25) search\n",
    "  * contextualized vs. non-contextualized chunks\n",
    "  * number of records retrieved (k=5, k=10, k=20)\n",
    "\n",
    "We calculate these metrics to evaluate retrieval:\n",
    "  * precision\n",
    "  * recall\n",
    "  * F1 score\n",
    "  * mean reciprocal rank (MRR)\n",
    "\n",
    "And for evaluating end-to-end performance:\n",
    "  * end-to-end accuracy (LLM-as-a-judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3ae6a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from textwrap import dedent\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from es import get_es_client\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "from goog import get_google_client, MODEL\n",
    "from pandas import DataFrame\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from rag import end_to_end_advanced, end_to_end_basic, retrieve_advanced, retrieve_basic\n",
    "from utils import retry, write_to_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9269d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(eval_data: list[dict], retrieval_fn: Callable, client: Elasticsearch, k: int, contextual: bool, similarity_threshold: float | None = None, start_message: str = '') -> dict[str, float]:\n",
    "    '''\n",
    "    Evaluate retrieval by comparing retrieved chunks to the golden chunk\n",
    "    for each query in the eval dataset\n",
    "    '''\n",
    "\n",
    "    print(start_message)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "\n",
    "    for item in tqdm(eval_data, desc='evaluating retrieval'):\n",
    "        query = item['query']\n",
    "        golden_chunk_id = item['chunk_id']\n",
    "\n",
    "        # retrieve chunks\n",
    "        retrieved_chunks = retrieval_fn(\n",
    "            client,\n",
    "            query,\n",
    "            k=k,\n",
    "            contextual=contextual,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        retrieved_chunk_ids = [chunk['_source']['chunk_id'] for chunk in retrieved_chunks]\n",
    "\n",
    "        # calculate metrics\n",
    "        # 1 if golden chunk in the retrieved chunks, 0 otherwise\n",
    "        true_positives = 1 if golden_chunk_id in retrieved_chunk_ids else 0\n",
    "        # precision: 1 if golden chunk is the only chunk retrieved, lower otherwise\n",
    "        precision = true_positives / len(retrieved_chunk_ids) if retrieved_chunk_ids else 0\n",
    "        # recall: 1 if golden chunk in the retrieved chunks, 0 otherwise\n",
    "        recall = true_positives / 1 # len(golden_chunk_ids), except we only have one golden chunk per query\n",
    "        # mean reciprocal rank: 1 / (position of golden chunk in the retrieved chunks), 0 if not present\n",
    "        mrr = 1 / (retrieved_chunk_ids.index(golden_chunk_id) + 1) if golden_chunk_id in retrieved_chunk_ids else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    # calculate averages\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_mrr = sum(mrrs) / len(mrrs)\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "\n",
    "    result = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'mrr': avg_mrr,\n",
    "        'f1': f1,\n",
    "        'total_queries': len(eval_data)\n",
    "    }\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "class EndToEndEvaluation(BaseModel):\n",
    "    is_correct: bool\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "def evaluate_end_to_end(eval_data: list[dict], generate_answer_fn: Callable, client_es: Elasticsearch, client_goog: genai.Client, k: int, contextual: bool, similarity_threshold: float | None = None, start_message: str = '') -> Tuple[float, list[dict]]:\n",
    "\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "\n",
    "    print(start_message)\n",
    "\n",
    "    for i, item in enumerate(tqdm(eval_data, desc='evaluating end-to-end')):\n",
    "        query = item['query']\n",
    "        correct_answer = item['answer']\n",
    "\n",
    "        generated_answer = retry(generate_answer_fn, (client_es, client_goog, query, k, contextual, similarity_threshold))\n",
    "\n",
    "        eval_prompt = dedent(f'''\n",
    "            You are an AI assistant tasked with evaluating the correctness of answers to questions about different Texas Instruments calculators.\n",
    "\n",
    "            Question: {query}\n",
    "\n",
    "            Correct Answer: {correct_answer}\n",
    "\n",
    "            Generated Answer: {generated_answer}\n",
    "\n",
    "            Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ.\n",
    "\n",
    "            Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct.\n",
    "\n",
    "            However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect.\n",
    "\n",
    "            Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "\n",
    "            Using the response schema provided, give your judgement on the correctness of the generated answer, and provide an explanation for your decision.\n",
    "        ''')\n",
    "\n",
    "        def _generate_eval(client: genai.Client, prompt: str) -> EndToEndEvaluation:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=GenerateContentConfig(\n",
    "                    temperature=0.0,  # minimum creativity\n",
    "                    response_mime_type='application/json',\n",
    "                    response_schema=EndToEndEvaluation,\n",
    "                ),\n",
    "            )\n",
    "            if response.parsed:\n",
    "                return response.parsed\n",
    "            print(f'Error parsing evaluation response for question {i}, assuming incorrect.')\n",
    "            return EndToEndEvaluation(is_correct=False, explanation=response.text)\n",
    "\n",
    "        evaluation = retry(_generate_eval, (client_goog, eval_prompt))\n",
    "\n",
    "        if evaluation.is_correct:\n",
    "            correct_answers += 1\n",
    "        results.append(evaluation.model_dump())\n",
    "\n",
    "        print(f'Question {i + 1}/{total_questions}: {query}')\n",
    "        print(f'Correct Answer: {correct_answer}')\n",
    "        print(f'Generated Answer: {generated_answer}')\n",
    "        print(f'Correct: {evaluation.is_correct}')\n",
    "        print('-'*88)\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f'Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}')\n",
    "\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(__file__).parent.parent\n",
    "eval_dataset = project_root / 'data' / 'tech-manual-rag.eval.jsonl'\n",
    "\n",
    "with open(eval_dataset, 'r') as f:\n",
    "    eval_data = [json.loads(line) for line in f]\n",
    "\n",
    "client_es = get_es_client()\n",
    "client_goog = get_google_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################### retrieval evaluation #########################\n",
    "df = DataFrame({'desc': [], 'eval': []})\n",
    "\n",
    "# basic retrieval, non-contextualized\n",
    "desc = 'basic retrieval, non-contextualized, k=20'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=20, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'basic retrieval, non-contextualized, k=10'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=10, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'basic retrieval, non-contextualized, k=5'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=5, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "# basic retrieval, contextualized\n",
    "desc = 'basic retrieval, contextualized, k=20'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=20, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'basic retrieval, contextualized, k=10'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=10, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'basic retrieval, contextualized, k=5'\n",
    "result = evaluate_retrieval(eval_data, retrieve_basic, client_es, k=5, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "write_to_jsonl(df.to_dict('records'), project_root / 'data' / 'eval-retrieval-basic.jsonl')\n",
    "\n",
    "df = DataFrame({'desc': [], 'eval': []})\n",
    "\n",
    "# advanced retrieval, non-contextualized\n",
    "desc = 'advanced retrieval, non-contextualized, k=20'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=20, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'advanced retrieval, non-contextualized, k=10'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=10, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'advanced retrieval, non-contextualized, k=5'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=5, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "# advanced retrieval, contextualized\n",
    "desc = 'advanced retrieval, contextualized, k=20'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=20, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'advanced retrieval, contextualized, k=10'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=10, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "desc = 'advanced retrieval, contextualized, k=5'\n",
    "result = evaluate_retrieval(eval_data, retrieve_advanced, client_es, k=5, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, result]\n",
    "\n",
    "write_to_jsonl(df.to_dict('records'), project_root / 'data' / 'eval-retrieval-advanced.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385cbd2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "######################### end-to-end evaluation #########################\n",
    "df = DataFrame({'desc': [], 'accuracy': []})\n",
    "\n",
    "# end-to-end, basic retrieval, non-contextualized\n",
    "desc = 'end-to-end, basic retrieval, non-contextualized, k=20'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=20, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, basic retrieval, non-contextualized, k=10'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=10, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, basic retrieval, non-contextualized, k=5'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=5, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "# end-to-end, basic retrieval, contextualized\n",
    "desc = 'end-to-end, basic retrieval, contextualized, k=20'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=20, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, basic retrieval, contextualized, k=10'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=10, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, basic retrieval, contextualized, k=5'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_basic, client_es, client_goog, k=5, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "write_to_jsonl(df.to_dict('records'), project_root / 'data' / 'eval-end-to-end-basic.jsonl')\n",
    "\n",
    "df = DataFrame({'desc': [], 'accuracy': []})\n",
    "\n",
    "# end-to-end, advanced retrieval, non-contextualized\n",
    "desc = 'end-to-end, advanced retrieval, non-contextualized, k=20'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=20, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, advanced retrieval, non-contextualized, k=10'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=10, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, advanced retrieval, non-contextualized, k=5'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=5, contextual=False, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "# end-to-end, advanced retrieval, contextualized\n",
    "desc = 'end-to-end, advanced retrieval, contextualized, k=20'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=20, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, advanced retrieval, contextualized, k=10'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=10, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "desc = 'end-to-end, advanced retrieval, contextualized, k=5'\n",
    "accuracy, results = evaluate_end_to_end(eval_data, end_to_end_advanced, client_es, client_goog, k=5, contextual=True, start_message=desc)\n",
    "df.loc[df.shape[0]] = [desc, accuracy]\n",
    "write_to_jsonl(results, Path(f'./data/eval-{desc.replace(',', '').replace(\" \", \"-\")}.jsonl'))\n",
    "\n",
    "write_to_jsonl(df.to_dict('records'), project_root / 'data' / 'eval-end-to-end-advanced.jsonl')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
