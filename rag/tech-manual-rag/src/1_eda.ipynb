{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96c20ec",
   "metadata": {},
   "source": [
    "# Exploratory Data Analyis\n",
    "\n",
    "The plan is to:\n",
    "* ingest pdfs, extract text, create jsonl dataset\n",
    "* view some basic stats on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13bbb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from goog import get_text_from_pdf_gcvision\n",
    "from text import get_text_from_pdf_pymupdf, chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b13eed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_pdfs(filepath: Path) -> Generator[dict, None, None]:\n",
    "    \"\"\"\n",
    "    Preprocess all pdfs in a directory and return a dictionary of documents\n",
    "    \"\"\"\n",
    "    for i, file in enumerate(sorted(filepath.iterdir())):\n",
    "        if file.is_file() and file.suffix == '.pdf':\n",
    "            print(f'preprocessing doc #{i + 1}')\n",
    "            pages = get_text_from_pdf_gcvision(file)\n",
    "            # pages = get_text_from_pdf_pymupdf(file)\n",
    "            if not pages or sum(len(page) for page in pages) == 0:\n",
    "                print(f'skipping doc #{i + 1} because it has no text')\n",
    "                continue\n",
    "            doc = {}\n",
    "            doc['pages'] = [{'page_text': page} for page in pages]\n",
    "            doc['num_pages'] = len(pages)\n",
    "            doc['text'] = '\\n'.join(pages)\n",
    "            doc['filename'] = file.name\n",
    "            doc['doc_id'] = i + 1\n",
    "            chunk_texts = chunk_text(doc['text'])\n",
    "            doc['chunks'] = [\n",
    "                {'chunk_text': t, 'embedding': [], 'chunk_id': f'{doc[\"doc_id\"]}-{j + 1}'} for j, t in enumerate(chunk_texts)\n",
    "            ]\n",
    "            doc['num_chunks'] = len(chunk_texts)\n",
    "            doc['chunks_contextualized'] = []\n",
    "            print(f'doc #{i + 1} preprocessing done ({len(chunk_texts)} chunks)')\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def ingest_pdfs_to_dataset(src: Path, dst: Path) -> None:\n",
    "    \"\"\"\n",
    "    Ingest all pdfs in a directory, preprocess them to extract text and chunks, save to jsonl file\n",
    "    \"\"\"\n",
    "    if not src.is_dir():\n",
    "        raise ValueError(f'Directory \"{src}\" does not exist.')\n",
    "    pdf_files = [f for f in sorted(src.iterdir()) if f.is_file() and f.suffix == '.pdf']\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f'No PDF files found in directory \"{src}\"')\n",
    "\n",
    "    # preprocess pdfs and yield a json document for each\n",
    "    # write data to jsonl file\n",
    "    progress = tqdm(desc='preprocessing pdfs', unit='docs', total=len(pdf_files), colour='#ccc2ff')\n",
    "    for doc in preprocess_pdfs(src):\n",
    "        # opening + closing the file so we can check on the progress more easily\n",
    "        with open(dst, 'a') as f:\n",
    "            f.write(json.dumps(doc) + '\\n')\n",
    "        progress.update(1)\n",
    "    print(f'done preprocessing {len(pdf_files)} pdfs')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b762b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(__file__).parent.parent\n",
    "data_dir = project_root / 'data'\n",
    "pdf_dir = data_dir / 'texas_instruments_manuals'\n",
    "dataset_orig = data_dir / 'tech-manual-rag.jsonl'\n",
    "dataset_contextualized = data_dir / 'tech-manual-rag.contextualized.jsonl'\n",
    "dataset_embedded = data_dir / 'tech-manual-rag.contextualized.embedded.jsonl'\n",
    "dataset_eval = data_dir / 'tech-manual-rag.eval.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08903c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ingest_pdfs_to_dataset(pdf_dir, dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(filepath: Path) -> dict:\n",
    "    \"\"\"\n",
    "    get a few basic statistics from the dataset\n",
    "    \"\"\"\n",
    "    total_docs = 0\n",
    "    total_pages = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            total_docs += 1\n",
    "            total_pages += doc['num_pages']\n",
    "            total_chunks += doc['num_chunks']\n",
    "    stats = {\n",
    "        'total_documents': total_docs,\n",
    "        'total_pages': total_pages,\n",
    "        'total_chunks': total_chunks\n",
    "    }\n",
    "    print(stats)\n",
    "    return stats\n",
    "\n",
    "get_dataset_stats(dataset_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d56165",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "{'total_documents': 40, 'total_pages': 9349, 'total_chunks': 8875}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
